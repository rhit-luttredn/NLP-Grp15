<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Long Document Summarization in a Low Resource Setting using Pretrained Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-03-01">1 Mar 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ahsaas</forename><surname>Bajaj</surname></persName>
							<email>abajaj@umass.edu</email>
						</author>
						<author>
							<persName><forename type="first">Pavitra</forename><surname>Dangati</surname></persName>
							<email>sdangati@umass.edu</email>
						</author>
						<author>
							<persName><forename type="first">Kalpesh</forename><surname>Krishna</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pradhiksha</forename><forename type="middle">Ashok</forename><surname>Kumar</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Massachusetts Amherst</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Massachusetts Amherst</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Massachusetts Amherst</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Massachusetts Amherst</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">University of Massachusetts Amherst</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">University of Massachusetts Amherst</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">Equal Contribution</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Long Document Summarization in a Low Resource Setting using Pretrained Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-03-01">1 Mar 2021</date>
						</imprint>
					</monogr>
					<idno type="MD5">FF183B474988358A62FA8F66558B53A0</idno>
					<idno type="arXiv">arXiv:2103.00751v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-05-08T21:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstractive summarization is the task of compressing a long document into a coherent short document while retaining salient information. Modern abstractive summarization methods are based on deep neural networks which often require large training datasets. Since collecting summarization datasets is an expensive and time-consuming task, practical industrial settings are usually low-resource. In this paper, we study a challenging low-resource setting of summarizing long legal briefs with an average source document length of 4268 words and only 120 available (document, summary) pairs. To account for data scarcity, we used a modern pretrained abstractive summarizer BART (Lewis et al., 2020), which only achieves 17.9 ROUGE-L as it struggles with long documents. We thus attempt to compress these long documents by identifying salient sentences in the source which best ground the summary, using a novel algorithm based on GPT-2 (Radford et al., 2019)  language model perplexity scores, that operates within the low resource regime. On feeding the compressed documents to BART, we observe a 6.0 ROUGE-L improvement. Our method also beats several competitive salience detection baselines. Furthermore, the identified salient sentences tend to agree with an independent human labeling by domain experts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction and Related Work</head><p>Text summarization is the task of generating a smaller coherent version of a document preserving key information. Typical abstractive summarization algorithms use seq2seq models with attention <ref type="bibr" target="#b2">(Chopra et al., 2016)</ref>, copy mechanisms <ref type="bibr" target="#b5">(Gu et al., 2016)</ref>, content selection <ref type="bibr" target="#b1">(Cheng and Lapata, 2016)</ref>, pointer-generator methods <ref type="bibr" target="#b18">(See et al., 2017)</ref> and reinforcement learning <ref type="bibr" target="#b22">(Wu and Hu, 2018)</ref>. These methods perform well in high resource summarization datasets with small documents such as CNN/DailyMail <ref type="bibr" target="#b12">(Nallapati et al., 2016)</ref>, Gigaword <ref type="bibr" target="#b17">(Rush et al., 2015)</ref>, etc. However, summarization over long documents with thousands of tokens is a more practically relevant problem. Existing solutions focus on leveraging document structure <ref type="bibr" target="#b3">(Cohan et al., 2018)</ref> or do mixed model summarization involving compression or selection followed by abstractive summarization <ref type="bibr" target="#b8">(Liu et al., 2018;</ref><ref type="bibr" target="#b4">Gehrmann et al., 2018)</ref>. However, these methods require large amounts of training data. Low resource settings are common in real world applications as curating domain specific datasets especially over long documents and on a large scale, is both expensive and time consuming.</p><p>A human summarizing a long document would first understand the text, then highlight the important information, and finally paraphrase it to generate a summary. Building on this intuition, we present a low-resource long document summariza- tion algorithm (Section 2) operating in 3 steps -(1) ground sentences of every training set summary into its source, identifying salient sentences;</p><p>(2) train a salience classifier on this data, and use it to compress the source document during test time;</p><p>(3) feed the compressed document to a state-of-theart abstractive summarizer pretrained on a related domain to generate a coherent and fluent summary.</p><p>To tackle data scarcity, we use pretrained language models in all three steps, which show strong generalization <ref type="bibr">(Devlin et al., 2019)</ref> and are sample efficient <ref type="bibr" target="#b23">(Yogatama et al., 2019)</ref>. Notably, our step (1) uses a novel method based on GPT-2 perplexity <ref type="bibr" target="#b16">(Radford et al., 2019)</ref> to ground sentences.</p><p>Unlike prior work <ref type="bibr" target="#b14">(Parida and Motlicek, 2019;</ref><ref type="bibr" target="#b10">Magooda and Litman, 2020)</ref> tackling data scarcity in summarization, our method needs no synthetic data augmentation. Moreover, we study a significantly more resource constrained setting -a complex legal briefs dataset (Section 2) with only 120 available (document, summary) pairs and an average of 4.3K tokens per document; <ref type="bibr" target="#b14">Parida and Motlicek (2019)</ref> assume access to 90,000 pairs with a maximum of 0.4K source document tokens, Magooda and Litman (2020) use 370 pairs with 0.2K source document tokens.</p><p>Despite this challenging setup, our method beats an abstractor-only approach by 6 ROUGE-L points, and also beats several competitive salience detection baselines (Section 3). Interestingly, identified salient sentences show agreement with an independent human labeling by domain experts, further validating the efficacy of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset and Approach</head><p>To mimic the real world scenario of summarization over long domain-specific documents, we curate 120 document-summary pairs from publicly available Amicus Briefs<ref type="foot" target="#foot_0">1</ref> , thus simulating the legal domain.<ref type="foot" target="#foot_1">2</ref> As shown in Table <ref type="table" target="#tab_0">1</ref>, our dataset is significantly smaller than the popular CNN/Daily Mail benchmark <ref type="bibr" target="#b12">(Nallapati et al., 2016)</ref>  To tackle this low resource setting, we use the state-of-the-art abstractive summarizer BART <ref type="bibr" target="#b7">(Lewis et al., 2020)</ref>, pretrained on a dataset from a related domain (CNN/DM). Since BART was trained on short documents, it truncates documents longer than 1024 subwords. Hence, instead of feeding the whole source document as input to BART, we feed salient sentences extracted using a salience classifier. Our salience classification dataset is built using a novel method which grounds summary sentences to sentences in source with language model perplexity scores. Our approach (Figure <ref type="figure" target="#fig_0">1</ref>) resembles the extract-then-abstract paradigm popular in prior work <ref type="bibr" target="#b4">(Gehrmann et al., 2018;</ref><ref type="bibr" target="#b8">Liu et al., 2018;</ref><ref type="bibr" target="#b19">Subramanian et al., 2019;</ref><ref type="bibr" target="#b0">Chen and Bansal, 2018)</ref>.</p><p>Extraction Stage: To extract the most important content from the source document required to generate the summary, we pose content selection as a binary classification task, labeling every sentence in the source document as salient or non-salient. Sentences classified as salient are concatenated in the order of occurrence in the source document 3 to generate a compressed "extractive summary", which is then fed to the abstractive summarizer.</p><p>In addition to identifying important information, the salience classifier is able to remove repetitive boilerplate text which is common in technical documents but often irrelevant to the actual content.</p><p>Training Data for Salience Classification: Since we do not have sentence-level training data for the classifier, we construct it by grounding sentences of the ground truth summary to sentences in the source document. Consider a source document S consisting of m sentences s 1:m and a target summary T consisting of n sentences t 1:n where m &gt;&gt; n. We compute the salience score for every source sentence s i ∈ S as 1 n n j=0 f (s i , t j ). Here f (s, t) is a measure of how much source sentence s grounds target sentence t. Following this, we sort the sentences in the source document based on salience score. The highest scoring 3n sentences are chosen as salient sentences and the lowest scoring 3n are chosen as non-salient sentences. 4 We construct our dataset for salience classification by running this algorithm for every (S, T ) pair in the training dataset. To ensure generalization with limited training data, we incorporate transfer learning and build our classifier by finetuning BERT-base (Devlin et al., 2019) using transformers <ref type="bibr" target="#b21">(Wolf et al., 2019)</ref>. More details on training are provided in Appendix A.2.</p><p>Choice of f (s, t): To measure how much a source sentence s grounds a target sentence t we measure the perplexity of t conditioned on s, using a pretrained language model GPT-2 large <ref type="bibr" target="#b16">(Radford et al., 2019)</ref>. More formally, we concatenate s and t as [s; t] and feed it as input to GPT-2 large, calculating perplexity over the tokens of t.</p><p>3 Maintaining the order of sentences ensures the logical flow of information is not disrupted.</p><p>4 3n is a tuned hyperparameter. Whenever m &lt; 6n, we sort the sentences according to the salience score and assign salient to the top half and non-salient to the bottom half.</p><p>Here, a lower perplexity corresponds to a higher f (s, t) score. We find that this measure correlates with entailment and outperforms other choices of f (s, t) like n-gram overlap, sentence embedding similarity &amp; entailment classifiers (Section 3.3).</p><p>Abstraction Stage: Having compressed the source document using our extractor, we use a black-box pretrained abstractive summarizer trained on a related domain. In this work, we make use of the state-of-the-art model (i.e. BART), which is based on pretrained language models. Pretraining on CNN/DM helps BART generalize to unseen but related domains like legal briefs.<ref type="foot" target="#foot_2">5</ref> 3 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Evaluating the extractor</head><p>To evaluate our proposed extractor, we first check whether our salience classifier generalizes to a heldout test set<ref type="foot" target="#foot_3">6</ref> . Indeed, it achieves a classification accuracy of 73.66%, and qualitative analysis of the classifications confirm its ability to identify boilerplate sentences as non-salient. Our classifier compresses source documents by 61% on average. <ref type="foot" target="#foot_4">7</ref>Next, we evaluate the quality of extracted salient sentences by checking the extent to which they overlap in information with the gold test set summaries, by measuring ROUGE-1/2 recall scores. As shown in Table <ref type="table">2</ref>, our extractor outperforms a random selection of the same number of sentences and is comparable to the upper-bound recall performance achieved by feeding in the whole source document. Finally, to measure the extent to which our salience classifier matches human judgement, domain experts identified 8-10 salient sentences in four test documents with more than 200 sentences each on request. Despite their scarcity, our salience classifier recovers 64.7% marked sentences, confirming correlation with human judgments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluating the entire pipeline</head><p>We evaluate the entire pipeline by measuring the quality of abstractive summaries, obtained by feeding the extractive summary to BART. We study two abstractor settings: (1) Treating BART as a black-box with no modification; (2) Finetuning Table <ref type="table">2</ref>: ROUGE-1/2 (R-1/2) recall scores of the gold summary with respect to the the "Source" document.</p><p>Our saliency-driven extractor performs better than a random selection of the same number of sentences and is close to the upperbound recall performance achieved by feeding in the whole source document.  BART on the training and validation split of Amicus dataset<ref type="foot" target="#foot_5">8</ref> . We present results on the Amicus test set. We compare our model against several competitive baselines -(1) NE: no extraction;</p><p>(2) Random: a random selection of the same number of sentences as our extractive summary;</p><p>(3) Tex-tRank <ref type="bibr" target="#b11">(Mihalcea and Tarau, 2004;</ref><ref type="bibr" target="#b8">Liu et al., 2018)</ref>: unsupervised graph based approach to rank text chunks within a document; (4) Bottom-up summarizer <ref type="bibr" target="#b4">(Gehrmann et al., 2018)</ref>: a strong extractthen-abstract baseline where content selection is posed as a word-level sequence tagging problem. Similar to our setting, their content selector also uses large pretrained models (ELMo, <ref type="bibr" target="#b15">Peters et al., 2018)</ref>, which we finetune on our training set.</p><p>As seen in Table <ref type="table" target="#tab_3">3</ref>, we observe a 4.8 / 6 ROUGE-1/L improvement when compared to the no extractor baseline (NE), and 2.3 / 3.2 R-1/L improvement over the strongest extractor baseline (per metric); confirming the effectiveness of our method. In addition, finetuning the CNN/DM pretrained BART on 96 Amicus documents helps in domain adaption and boosts the ROUGE scores of both baselines and our method (f.t. BART). Specifically, we ob- Table <ref type="table">4</ref>: Results of our extract-then-abstract pipeline (after finetuning BART) by varying f (s, t). Our choice of GPT-2 perplexity performs better than 3 alternatives.</p><p>serve a 2.1 / 0.5 R-1/L boost in performance and outperform the best baseline (per metric) by 2.0 / 1.0 R-1/L points. Our model's improvements are statistically significant (p-value&lt; 0.06) except for when comparing our extractor + f.t BART with Bottom-up + f.t BART, the p-value is 0.16 due to the small test set. Refer Appendix A.3 for qualitative analysis of our proposed model's generations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Validating the choice of f (s, t)</head><p>In Section 2 we used GPT-2 perplexity scores to measure the extent to which a source sentence grounds a target sentence. To motivate this choice, we measure its correlation with existing entailment datasets. We randomly sample 5000 sentences from each class of the MultiNLI dataset <ref type="bibr" target="#b20">(Williams et al., 2018)</ref> and compute the perplexity of the hypothesis with the premise as context. As seen in Figure <ref type="figure">2</ref>, entailment pairs tend to have the lowest perplexity. This motivates our choice of f (s, t), since hypothesis sentences are best grounded in premise sentences for entailment pairs.<ref type="foot" target="#foot_7">9</ref> To further validate the merit of GPT-2 perplexity, we conduct ablations using alternatives for f (s, t):</p><p>(1)</p><p>Entailment score from a RoBERTa based MNLI classifier <ref type="bibr" target="#b9">(Liu et al., 2019)</ref> (2) Cosine similarity of averaged embeddings from final layer of <ref type="bibr">BERT (Devlin et al., 2019)</ref> (3) BLEU scores <ref type="bibr" target="#b13">(Papineni et al., 2002)</ref>. We present ROUGE scores using our whole extract-then-abstract pipeline with different choices of f (s, t) in Table <ref type="table">4</ref>. We note that perplexity performs the best, 2.4 ROUGE-1 better than the best alternative and also performs 3.41 ROUGE-1 better than entailment. 10</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We tackle an important real-world problem of summarizing long domain-specific documents with very less training data. We propose an extract-thenabstract pipeline which uses GPT-2 perplexity and a BERT classifier to estimate sentence salience. This sufficiently compresses a document, allowing us to use a pretrained model (BART) to generate coherent &amp; fluent summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A.1 Data pre-processing</p><p>In this section, the various pre-processing steps of data performed at different stages are explained.</p><p>Extracting (document, summary) pairs: The 120 pairs of Amicus Briefs were scrapped from their website<ref type="foot" target="#foot_8">11</ref> . The Summary of Arguments section of the Amicus Briefs was extracted as the target summary and the main content excluding title page, table of contents, acknowledgements, appendix etc was extracted as document/source.</p><p>Sentence pre-processing: Sentences the (document, summary) files were split using the spaCy<ref type="foot" target="#foot_9">12</ref> sentence splitter. Furthermore, the sentences were each processed to remove special characters using regex rules. If a sentence contained less that 5 words, it was pruned out from the computation of f (s, t) to reduce the complexity of pairs considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Sentence Saliency Classifier</head><p>Training Details:</p><p>Our classifier uses the BERT sequence labeling configuration<ref type="foot" target="#foot_10">13</ref> from transformers <ref type="bibr" target="#b21">(Wolf et al., 2019)</ref>, which is a pretrained BERT-base model with an initially untrained classification head on the [CLS] feature vector. This model is then finetuned for 5 epochs using the training data which consists of 5363 sentences in the Amicus dataset (equal distribution among the two classes). We use a train / dev / test split of 60%, 20%, 20%. Training configuration of the classifier is as follows: learning rate = 2e-5, max grad norm = 1.0, num training steps = 1000, num warmup steps = 100, warmup proportion = 0.1, Optimizer = Adam, Schduler = linear with warmup.</p><p>Alternate methods to choose +/-samples: The aggregate scoring method mentioned in Section 2 was one choice to pick salient and non-salient samples for each document. Aggregate method compresses the source by 61% on an average. The other methods experimented were:</p><p>• Top k -Bottom k: ∀t j ∈ T, we picked the top-k scoring source sentences as positive samples and the bottom-k sentences as the negative samples ensuring that {positive} ∩ {negative} = 0. Using this technique, the classifier achieves accuracy of nearly 1 as can be seen from Table <ref type="table" target="#tab_6">5</ref>. On qualitative analysis, we identified that there is a clear distinction in the positive and the negative examples. Eg: sentences such as 'This document is prepared by XYZ' would be picked as non salient sentence and classifier is able to achieve high accuracy. This could however be used to train a classifier to identify boiler plate sentences across the document. This method compresses source document by 63% on an average.</p><p>• Random negative sampling: Salient examples were chosen for a document as per the above method. For the non salient examples, we randomly sampled from the rest of the document. This allows the classifier to learn about sentences that that are difficult to be classified as positive or negative. Hence, the accuracy of the classifier is lower than the other two methods as can be seen from Table <ref type="table" target="#tab_6">5</ref>. This method compresses the source document by 70% on an average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compute time and resources:</head><p>Execution time for different choice of f(s,t) for all 120 pairs:</p><p>• Perplexity using GPT-2:executes within 15hrs using 2 GPUs</p><p>• Entailment score using RoBERTa: executes within 22hrs using 2 GPUs</p><p>• Cosine Similarity using BERT [CLS] embeddings: executes within 3hrs on a single GPU • BLEU score using nltk: executes within 15min on a single GPU.</p><p>These scores need to be generated once and can be reused for various experiments. Sampling methods to choose salient and non-salient sentences for each document takes less than a minute to run.</p><p>Analysis: (a) Table <ref type="table" target="#tab_6">5</ref> shows the classifier accuracies for combinations of f(s,t) and sampling methods. We observe that for the aggregate sampling method, although perplexity based classifier does not have the highest accuracy, our  pipeline where f (s, t) is perplexity score gives the best result(ROUGE) amongst the ablation experiments(Table <ref type="table">4</ref>). Classifier accuracy is determined on automated labelling based on the saliency score, rather than true labels, hence best classifier does not imply best summarization. (b) Table <ref type="table">6</ref> shows the examples of using perplexity as f(s,t) to see how the summary grounds the source. The table shows three summary sentences and the corresponding source sentences that had the lowest perplexity scores. We can see that, summary either has a similar meaning or logically follows the source. (c) Table <ref type="table" target="#tab_7">7</ref> has three examples each for salient sentences and non-salient sentences inferred by the classifier trained on data prepared as mentioned in Section 2. The third sentence in the non-salient sentences column is an example of boiler-plate content detected that is present across documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Abstractive Summarizer: BART</head><p>BART is a seq2seq model based on denoising pretraining objective which is supposed to generalize better on various natural language understanding tasks; abstractive summarization being one of them.</p><p>For abstractive stage of our proposed approach, we decided to see (bart.large.cnn) variant which is essentially BART-large model (with 12 encoder and decoder layers and 400 million parameters) finetuned for CNN/DM summarization task. We use the pre-computed weights available for use here<ref type="foot" target="#foot_11">14</ref> . Using BART's text generation script, we set length penalty (lenpen) as 2.0 and minimum length (min len) as 500 words in order to encourage BART to produce longer outputs which is more suitable to our dataset. Also, we use beam size of 4 and and no repeat ngram size of 3.</p><p>Finetuning: We use the train and dev splits of Amicus dataset (96 source-target pairs) and finetune BART for summarization task starting from its CNN/DM finetuned checkpoint. First, we pre-process the dataset as per the guidelines in the official code<ref type="foot" target="#foot_12">15</ref> . We finetune for 500 epochs with learning rate of 3e-5 and early stop if validation loss doesn't decrease for 50 epochs. Others parameters are as follows: total num updates = 20000, warmup updates = 500, update freq = 4, optimiser = Adam with weight decay of 0.01. Rest of parameters were kept as default in the official script. Results (Precision, Recall, F1) on the test set of Amicus using the existing BART model and finetuned BART are shown in Table <ref type="table" target="#tab_8">8</ref>. Table <ref type="table">9</ref> shows an example of target summary and summary generated by our model(Section 2) for one sample source document. We can see that the summary generated by our model is fluent and has coherent flow of information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary Sentence Source Sentence</head><p>In the immigration context, this jurisprudence has prompted the Court to reject the notion that the so-called entry fiction is of constitutional significance.</p><p>Prior to Knauff and Mezei, the distinction between noncitizens who had entered the United States and those who remained outside it had not had been elevated to a bright-line constitutional rule, and entry had never been completely determinative of the fact or extent of protection under the Due Process Clause.</p><p>It has accordingly authorized such detention only in limited circumstances pursuant to a carefully defined scheme. The Court's substantive due process jurisprudence also recognizes that an individual may be subjected to regulatory detention only in narrow circumstances under a carefully drawn scheme.</p><p>With respect to substantive due process, this Court has increasingly recognized the punitive consequences of indefinite regulatory detention.</p><p>Thus, the Court has substantially restricted the availability and duration of regulatory confinement in the -years since it decided Meze1.In Zadvydas, this Court established that its substantive due process jurisprudence provided the appropriate framework for evaluating the administrative detention of noncitizens pending removal from the United States.</p><p>Table <ref type="table">6</ref>: Using GPT-2 perplexity as f(s,t), here are three sentences from the summary with corresponding source sentence, having the lowest perplexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Salient Sentences</head><p>Non-Salient sentences The same time, the Court has long been skeptical of the military's authority to try individuals other than active service personnel.</p><p>A government predicated on checks and balances serves not only to make Government accountable but also to secure individual liberty. On the basis of this revised test, the Court of Appeals refused to apply the exceptional circumstances exception to Al-Nashiri's petition.</p><p>At present, the Rules for Courts-Martial require that the accused be brought to trial within 120 days after the earlier of preferral of charges or confinement. Consonant with that tradition, this Court should review the Court of Appeals' decision to confirm that exceptional delay before trial remains of central concern on habeas review and is indeed one of the very dangers the writ of habeas corpus was designed to avoid.   <ref type="bibr">., 146 N.H..295, 298 (2001)</ref>). Because there is no dispute that the fundamental right to parent isat stake in abuse and neglect proceedings, the ABA focuses its discussion on the second and third factors of the three factor test.As to the second, so-called "risk of error" factor, the ABA's conclusion, after years of investigation and analysis, is that the absence of counsel for indigent parent-defendants in abuse and neglect proceedings results in a significant risk of an erroneous determination. This is especially true where the opposing party is the State. As to the third, state's interest factor, the ABA's investigation shows that the interests of both the parent and the state are best served where indigent parent-defendants are represented. The ABA respectfully suggests that the evidence and analysis relevant to these two factors is so compelling in most, if not all, abuse and neglect proceedings involving indigent parent-defendants, that a case-by-case balancing of the factors should be rejected in favor of a rule requiring the appointment of counsel] for indigent parent-defendants in all such proceedings. The evidence and analysis supporting the ABA's policy includes the fact that a substantial majority of states have recognized an unqualified right to counsel for indigentparent-defendants in child custody proceedings.</p><p>Similarly, other industrial democraciesprovide indigent parent-defendants with such right to counsel. The ABA respectfully submits that this Court should require no less as a matter of due process under the New Hampshire Constitution.Although of whetherJn re Shelby R. resulted in a or not a natural parent'splurality role inruling, the the familyCourt is awas not split fundamentalon the libertyquestion interestprotected by the State Constitution. See In re Shelby R., 148 NH. at 244 (dissenting opinion). Hampshire constitution requires this court to determine whether indigentparents have a legally protected interest.</p><p>Most indigent parent -defendants are incapable of performingthe advocacy functions required in abuse and neglectproceedings. Most unrepresented parents cannot perform the advocacy functions --including investigating facts , making an orderly factual presentation , and cross -examining witnesses --that are required. The intense, emotionally charged backdrop against which custody decisionsare often made further exacerbates the inherent disadvantages faced by unrepresented indigent parents. The need for counsel for the indigentparent -defendant is especially great where the opposing party is the state. The court must weighthree factors : ( 1) the private interests that will be affected. ( 2) the risk of erroneousdeprivation of the liberty interest through the procedures used and the value , if any, ofadditional or substitute procedural safeguards. ( 3) the state ' s interest , including the function involved and fiscal and administrative burdens that additional or substituteprocedural requirements would entail id at 240 ; see also in re father , 155 n . h . <ref type="bibr">93 , 95 ( 2007 )</ref> . this court has previously concluded as to the first factor that adversary child custody proceedings implicate a fundamental liberty interest --the right to parent in this case, the central question thus becomes whether that right is sufficiently protected. The conclusion that counsel must be provided is so compelling in most , if not all cases , that a case -by -case balancing of the factors should be rejected in favor of a rule requiring the appointment of counsel for lowincome parent -defendant in all such proceedings to be constitutionally acceptable. The state is not the only adversary finding the only meaningful right to be heard when her adversary is not represented by counsel is not spaled against the traditional weapons of the state, such as the state's attorney general. The courts must also weigh the public interest in the child custody case, including the function involved and the cost of additional or substitute safeguards, as well as the cost to the state of the additional or substituted safeguards. The risk of an erroneous deprivation of the findamentalright to parent only increases the only increase in the risk that the state will find the child is not heard when the state is the adversary. The public interest is only increased by the fact that the child will not be heard by the state when the parent is represented by a lawyer. The high level of complexity of child custody cases makes it difficult for the court to make a fair and just decision.</p><p>Table <ref type="table">9</ref>: The table shows the comparison of summaries where the top summary is the target summary and the bottom summary is the one generated by our extractor and f.t BART. As we can see, the summary is coherent and has fluent information flow.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Our method for long document summarization task in low resource setting. The Extraction Model generates a compressed document D by identifying salient sentences. It is trained by computing salience score for each training set source sentence. The pretrained abstractive summarizer takes as input the compressed document.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Respectfully submitted, May 31, 2017 LINDA A. KLEIN Counsel of Record AMERICAN BAR ASSOCIATION 321 North Clark Street Chicago ...</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>and has significantly longer documents and summaries.</figDesc><table><row><cell>CNN/DM</cell><cell>312084</cell><cell>781</cell><cell>56</cell></row><row><cell>Amicus</cell><cell>120</cell><cell>4268</cell><cell>485</cell></row></table><note>Dataset# (S, T ) Avg. |S| Avg. |T | A comparison between the Amicus legal briefs dataset and the popular CNN/Daily Mail benchmark. Amicus has far fewer document-summary pairs #(S, T ), with more documents tokens (Avg. |S|) and summary tokens (Avg. |T |) on average.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Comparison of our method on the Amicus dataset with strong baselines. Our method outperforms all baselines in both Abstractor settings: (1) a pretrained CNN/DM BART; (2) the pretrained CNN/DM BART finetuned on the Amicus dataset (f.t. BART).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Perplexity distribution of the hypothesis given the premise for each of the three classes sampled from the MultiNLI dataset. Entailment pairs tend to have lower perplexity, validating our choice of f (s, t).</figDesc><table><row><cell></cell><cell></cell><cell cols="5">Perplexity distribution for classes on an MNLI dataset.</cell></row><row><cell>Percentage of examples</cell><cell>0.02 0.04 0.06 0.08</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>entailment contradiction neutral</cell></row><row><cell></cell><cell>0.00</cell><cell>0</cell><cell>5</cell><cell cols="2">10 Perplexity scores 15</cell><cell>20</cell><cell>25</cell></row><row><cell cols="3">Figure 2: Choice of f (s, t)</cell><cell></cell><cell></cell><cell>R-1</cell><cell>R-2</cell><cell>R-L</cell></row><row><cell cols="7">Entailment (using RoBERTa) 43.66 16.95 23.24</cell></row><row><cell cols="5">Similarity (using BERT)</cell><cell cols="2">44.67 16.69 23.81</cell></row><row><cell cols="3">BLEU (using nltk)</cell><cell></cell><cell></cell><cell cols="2">43.95 17.38 23.69</cell></row><row><cell cols="5">Perplexity (using GPT-2)</cell><cell cols="2">47.07 17.64 24.40</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>The accuracy of the held out set of Amicus for different classifiers trained on the data prepared using choice of different f(s,t) and sampling methods. Here, k=3.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>This table shows the sentences classified as salient and non-salient from one Amicus source document. We can see that the last sentence in the non-salient sentences column shows an example of boiler-plate content present across documents. The classifier is trained on data chosen on aggregate score of source sentences where f(s,t) is GPT-2 perplexity.</figDesc><table><row><cell cols="2">Metric</cell><cell cols="4">BART Ours + BART f.t. BART Ours + f.t. BART</cell></row><row><cell></cell><cell>Recall</cell><cell>40.87</cell><cell>47.46</cell><cell>46.90</cell><cell>56.04</cell></row><row><cell>ROUGE-1</cell><cell>Precision</cell><cell>47.21</cell><cell>49.97</cell><cell>48.68</cell><cell>46.16</cell></row><row><cell></cell><cell>F-1</cell><cell>40.17</cell><cell>44.97</cell><cell>43.47</cell><cell>47.07</cell></row><row><cell></cell><cell>Recall</cell><cell>13.76</cell><cell>16.54</cell><cell>17.84</cell><cell>21.50</cell></row><row><cell>ROUGE-2</cell><cell>Precision</cell><cell>15.46</cell><cell>17.04</cell><cell>17.84</cell><cell>17.10</cell></row><row><cell></cell><cell>F-1</cell><cell>13.36</cell><cell>15.37</cell><cell>16.30</cell><cell>17.64</cell></row><row><cell></cell><cell>Recall</cell><cell>18.34</cell><cell>25.58</cell><cell>21.30</cell><cell>29.62</cell></row><row><cell>ROUGE-L</cell><cell>Precision</cell><cell>21.04</cell><cell>26.27</cell><cell>21.35</cell><cell>23.47</cell></row><row><cell></cell><cell>F-1</cell><cell>17.95</cell><cell>23.95</cell><cell>19.35</cell><cell>24.40</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Overall pipeline results by adding our extractor (f(s,t) as GPT-2 perplexity + Classifier) to BART and finetuned BART (f.t. BART), including the precision and recall values for each metric.</figDesc><table><row><cell>This Court's determination of whether due process under the New HampshireConstitution requires</cell></row><row><cell>court-appointed counsel for indigent parent-defendants, in order to protect their fundamental right</cell></row><row><cell>to parent, requires the balancing of three factors-(1) theprivate interest at stake, (2) the risk of error</cell></row><row><cell>and the value of procedural safeguards, and (3)the state's interest. See In re Shelby R., 148 N.H. 237,</cell></row><row><cell>240 (2002) (citing In re Richard A</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://publichealthlawcenter.org/ amicus-briefs</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">The source contains detailed arguments that the court should consider for a case; the target summarizes them.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2">Details on our BART setup are provided in Appendix A.3.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3">Classifier data statistics at salient/non-salient sentences level:(Train=5363, Dev=1870, Test=2070)   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4">Note that classifier score can be thresholded to obtain more or less compression depending on domain and end-task.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5">The training and validation splits together comprise of</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="96" xml:id="foot_6">documents. The test split was not used.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7">We hypothesize contradiction sentences have slightly lower perplexity than neutral due to more word overlap.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_8">https://publichealthlawcenter.org/amicus-briefs</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_9">https://pypi.org/project/spacy/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_10">https://huggingface.co/transformers/ model_doc/bert.html#transformers. BertForSequenceClassification</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_11">https://github.com/pytorch/fairseq/ tree/master/examples/bart</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_12">https://github.com/pytorch/fairseq/ blob/master/examples/bart/README. summarization.md</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting</title>
		<author>
			<persName><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p18-1063</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
		<title level="s">Long Papers</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural Summarization by Extracting Sentences and Words</title>
		<author>
			<persName><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p16-1046</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
		<title level="s">Long Papers</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="484" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Abstractive Sentence Summarization with Attentive Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n16-1012</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="93" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A discourse-aware attention model for abstractive summarization of long documents</title>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soon</forename><surname>Doo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seokhwan</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazli</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><surname>Goharian ; Gururangan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics. 10 We hypothesize that RoBERTa overfits on the MNLI dataset that also has known biases</title>
		<title level="s">the Association for Computational Linguistics: Human Language Technologies</title>
		<editor>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</editor>
		<meeting><address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018. 2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="615" to="621" />
		</imprint>
	</monogr>
	<note>BERT: Pre-training of deep bidirectional transformers for language understanding. In Conference of the North American Chapter of the Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bottom-Up Abstractive Summarization</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d18-1443</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Incorporating Copying Mechanism in Sequence-to-Sequence Learning</title>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><forename type="middle">O K</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p16-1154</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
		<title level="s">Long Papers</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1631" to="1640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Annotation Artifacts in Natural Language Inference Data</title>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n18-2017</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</title>
				<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Bart</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">Marjan Ghazvininejad,. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etienne</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Pot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Goodrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><surname>Shazeer</surname></persName>
		</author>
		<title level="m">Generating wikipedia by summarizing long sequences</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mitigating Data Scarceness through Data Synthesis, Augmentation and Curriculum for Abstractive Summarization</title>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Magooda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diane</forename><surname>Litman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-emnlp.175</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
				<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">PageRank on semantic networks, with application to word sense disambiguation</title>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Tarau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Figa</surname></persName>
		</author>
		<idno type="DOI">10.3115/1220355.1220517</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on Computational Linguistics - COLING &apos;04</title>
				<meeting>the 20th international conference on Computational Linguistics - COLING &apos;04<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="404" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond</title>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cicero</forename><surname>Dos Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/k16-1028</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
				<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="280" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">BLEU</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics - ACL &apos;02</title>
				<meeting>the 40th Annual Meeting on Association for Computational Linguistics - ACL &apos;02<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Abstract Text Summarization: A Low Resource Challenge</title>
		<author>
			<persName><forename type="first">Shantipriya</forename><surname>Parida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petr</forename><surname>Motlicek</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d19-1616</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5994" to="5998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep Contextualized Word Representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n18-1202</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A Neural Attention Model for Abstractive Sentence Summarization</title>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d15-1044</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Get To The Point: Summarization with Pointer-Generator Networks</title>
		<author>
			<persName><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p17-1099</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
		<title level="s">Long Papers</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">On extractive and abstractive neural document summarization with transformer language models</title>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Pilault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n18-1101</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</title>
				<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-Art Natural Language Processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
		<idno>ArXiv, abs/1910.03771</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to Extract Coherent Summary via Deep Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v32i1.11987</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<title level="j" type="abbrev">AAAI</title>
		<idno type="ISSN">2159-5399</idno>
		<idno type="ISSNe">2374-3468</idno>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018-04-27">2018</date>
			<publisher>Association for the Advancement of Artificial Intelligence (AAAI)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyprien</forename><surname>De Masson D'autume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerome</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Chrzanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11373</idno>
		<title level="m">Learning and evaluating general linguistic intelligence</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
