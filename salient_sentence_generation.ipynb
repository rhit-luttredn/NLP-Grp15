{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "import pandas as pd\n",
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>article</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gr-qc0101015</td>\n",
       "      <td>[there is considerable current interest in stu...</td>\n",
       "      <td>[in this paper we consider the collision of sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0803.1640</td>\n",
       "      <td>[the first data system requiring dark energy c...</td>\n",
       "      <td>[upcoming weak lensing surveys can be used to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1510.01821</td>\n",
       "      <td>[quantum key distribution qkd is the first mat...</td>\n",
       "      <td>[the fully symmetric gaussian tripartite entan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1105.2448</td>\n",
       "      <td>[the active galactic nucleus agn unification s...</td>\n",
       "      <td>[x ray unabsorbed seyfert 2 galaxies appear to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1602.04433</td>\n",
       "      <td>[deep neural networks have significantly impro...</td>\n",
       "      <td>[the recent success of deep neural networks re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     article_id                                            article  \\\n",
       "0  gr-qc0101015  [there is considerable current interest in stu...   \n",
       "1     0803.1640  [the first data system requiring dark energy c...   \n",
       "2    1510.01821  [quantum key distribution qkd is the first mat...   \n",
       "3     1105.2448  [the active galactic nucleus agn unification s...   \n",
       "4    1602.04433  [deep neural networks have significantly impro...   \n",
       "\n",
       "                                            abstract  \n",
       "0  [in this paper we consider the collision of sp...  \n",
       "1  [upcoming weak lensing surveys can be used to ...  \n",
       "2  [the fully symmetric gaussian tripartite entan...  \n",
       "3  [x ray unabsorbed seyfert 2 galaxies appear to...  \n",
       "4  [the recent success of deep neural networks re...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_df = pd.read_csv('data/arxiv_cleaned.csv', converters={'article': pd.eval, 'abstract': pd.eval})\n",
    "sentence_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_id = 'gpt2-large'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_id).to(device)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_id, sep_token=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(s,t):\n",
    "    # Concatenate source and target sentences and encode them\n",
    "    sentence = f\"{s}; {t}\"\n",
    "    encodings = tokenizer(sentence, return_tensors='pt').to(device)\n",
    "    input_ids = encodings.input_ids.to(device)\n",
    "    semicolon_idx = (input_ids[0] == tokenizer.sep_token_id).nonzero(as_tuple=True)[0]\n",
    "\n",
    "    # Run GPT-2 large on the concatenated sentence\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "    \n",
    "    # Chop off the source sentence and the semicolon tokens\n",
    "    t_input_ids = input_ids[:, semicolon_idx+1:]\n",
    "    t_logits = outputs.logits[:,semicolon_idx+1:,:]\n",
    "\n",
    "    # Get the log probability of each token in the target sentence\n",
    "    # The uncommented bit gets the log softmax of the logits and then selects the log probability of the target token\n",
    "    # The commented bit gets the logits of the target token and then gets the log softmax\n",
    "    # Each method is not equivalent but I *think* the first is correct, the end results are similar regardless\n",
    "    t_log_probs = torch.nn.functional.log_softmax(t_logits, dim=-1)\n",
    "    t_log_probs = torch.gather(t_log_probs, dim=-1, index=t_input_ids.unsqueeze(-1)).squeeze(-1)\n",
    "    # t_logits = torch.gather(t_logits, dim=-1, index=t_input_ids.unsqueeze(-1)).squeeze(-1)\n",
    "    # t_log_probs = torch.nn.functional.log_softmax(t_logits, dim=-1)\n",
    "    t_log_probs = t_log_probs.view(-1)\n",
    "\n",
    "    # Get the perplexity of the target sentence\n",
    "    log_prob_sum = torch.sum(t_log_probs)\n",
    "    perplexity = torch.pow(1/-log_prob_sum, 1/len(t_input_ids[0])).item()\n",
    "    return perplexity\n",
    "\n",
    "def salience_score(s, target_summary):\n",
    "    # Get the perplexity of the source sentence with respect to each target sentence\n",
    "    # perplexities = target_summary.apply(lambda t: f(s, t))\n",
    "    perplexities = [f(s, t) for t in target_summary]\n",
    "\n",
    "    # Get the average perplexity of the source sentence\n",
    "    avg_perplexity = mean(perplexities)\n",
    "    return -avg_perplexity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on a example document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "astro-ph0011254\n",
      "106\n",
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['we report the first wide field mapping of the kinematics and stellar populations in the e3 galaxy ngc4365 .',\n",
       " 'the velocity maps extend previous long slit work .',\n",
       " 'they show two independent kinematic subsystems : the central @xmath0 pc rotates about the projected minor axis , and the main body of the galaxy , @xmath1 kpc , rotates almost at right angles to this .',\n",
       " 'the line strength maps show that the metallicity of the stellar population decreases from a central value greater than solar , to one half solar at a radius of 2 kpc .',\n",
       " 'the decoupled core and main body of the galaxy have the same luminosity weighted age , of @xmath214 gyr , and the same elevated magnesium iron ratio .',\n",
       " 'the two kinematically distinct components have thus shared a common star formation history .',\n",
       " 'we infer that the galaxy underwent a sequence of mergers associated with dissipative star formation that ended @xmath312 gyr ago .',\n",
       " 'the misalignment between the photometric and kinematic axes of the main body is unambiguous evidence of triaxiality .',\n",
       " 'the similarity of the stellar populations in the two components suggests that the observed kinematic structure has not changed substantially in 12 gyr .']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = sentence_df.loc[210]\n",
    "print(example.article_id)\n",
    "print(len(example.article))\n",
    "print(len(example.abstract))\n",
    "example.abstract"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Source Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the existence of decoupled cores in @xmath230 of the early type galaxies is strong evidence that mergers play an important part in the evolution of these systems @xcite .; we report the first wide field mapping of the kinematics and stellar populations in the e3 galaxy ngc4365 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8044186234474182"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test f(s,t) on an example sentence\n",
    "source = example['article'][0]\n",
    "target = example['abstract'][0]\n",
    "sentence = f\"{source}; {target}\"\n",
    "print(sentence.replace('\\n', ''))\n",
    "f(source, target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All source sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "saliency_scores = []\n",
    "for s in example['article']:\n",
    "    saliency_score = salience_score(s, test_doc['abstract'])\n",
    "    saliency_scores.append({\"sentence\": s, \"saliency_score\": saliency_score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max saliency score: -0.7958607739872403\n",
      "Min saliency score: -0.7999491956498888\n",
      "Average saliency score: -0.7972548668364559\n"
     ]
    }
   ],
   "source": [
    "max_saliency_score = max(saliency_scores, key=lambda x: x['saliency_score'])\n",
    "print(f\"Max saliency score: {max_saliency_score['saliency_score']}\")\n",
    "min_saliency_score = min(saliency_scores, key=lambda x: x['saliency_score'])\n",
    "print(f\"Min saliency score: {min_saliency_score['saliency_score']}\")\n",
    "avg_saliency_score = sum([s['saliency_score'] for s in saliency_scores])/len(saliency_scores)\n",
    "print(f\"Average saliency score: {avg_saliency_score}\")\n",
    "# print(saliency_scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on another document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0710.5721\n",
      "188\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "example = sentence_df.loc[125]\n",
    "print(example.article_id)\n",
    "print(len(example.article))\n",
    "print(len(example.abstract))\n",
    "# example.abstract"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Source Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in theoretical cosmology , many insights can already be gained from spatially isotropic friedmann robertson walker models @xmath0 with @xmath1 or @xmath2 .; the equation of state for radiation is derived in a canonical formulation of the electromagnetic field .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7509577870368958"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test f(s,t) on an example sentence\n",
    "source = example['article'][0]\n",
    "target = example['abstract'][0]\n",
    "sentence = f\"{source}; {target}\"\n",
    "print(sentence.replace('\\n', ''))\n",
    "f(source, target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Source Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m sentences:\n\u001b[0;32m      3\u001b[0m     s\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     saliency_score \u001b[39m=\u001b[39m salience_score(s, test_doc[\u001b[39m'\u001b[39;49m\u001b[39mabstract\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m      5\u001b[0m     saliency_scores\u001b[39m.\u001b[39mappend({\u001b[39m\"\u001b[39m\u001b[39msentence\u001b[39m\u001b[39m\"\u001b[39m: s, \u001b[39m\"\u001b[39m\u001b[39msaliency_score\u001b[39m\u001b[39m\"\u001b[39m: saliency_score})\n",
      "Cell \u001b[1;32mIn[4], line 35\u001b[0m, in \u001b[0;36msalience_score\u001b[1;34m(s, target_summary)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[39m# Get the perplexity of the source sentence with respect to each target sentence\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m target_summary:\n\u001b[1;32m---> 35\u001b[0m     perplexity \u001b[39m=\u001b[39m f(s, t)\n\u001b[0;32m     36\u001b[0m     total_perplexity \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m perplexity\n\u001b[0;32m     37\u001b[0m \u001b[39m# Get the average perplexity of the source sentence\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m, in \u001b[0;36mf\u001b[1;34m(s, t)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39m# Run GPT-2 large on the concatenated sentence\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> 10\u001b[0m     outputs \u001b[39m=\u001b[39m model(input_ids, labels\u001b[39m=\u001b[39;49minput_ids)\n\u001b[0;32m     12\u001b[0m \u001b[39m# Chop off the source sentence and the semicolon tokens\u001b[39;00m\n\u001b[0;32m     13\u001b[0m t_input_ids \u001b[39m=\u001b[39m input_ids[:, semicolon_idx\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m:]\n",
      "File \u001b[1;32mc:\\Users\\luttredn\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\luttredn\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1076\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1068\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1070\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[0;32m   1071\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[0;32m   1073\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1074\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1076\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[0;32m   1077\u001b[0m     input_ids,\n\u001b[0;32m   1078\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1079\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1080\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1081\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1082\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1083\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1084\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1085\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[0;32m   1086\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1087\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1088\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1089\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1090\u001b[0m )\n\u001b[0;32m   1091\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1093\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\luttredn\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\luttredn\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:900\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    890\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    891\u001b[0m         create_custom_forward(block),\n\u001b[0;32m    892\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    897\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    898\u001b[0m     )\n\u001b[0;32m    899\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 900\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[0;32m    901\u001b[0m         hidden_states,\n\u001b[0;32m    902\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[0;32m    903\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    904\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[0;32m    905\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m    906\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[0;32m    907\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    908\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    909\u001b[0m     )\n\u001b[0;32m    911\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    912\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\luttredn\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\luttredn\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:390\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    388\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[0;32m    389\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(hidden_states)\n\u001b[1;32m--> 390\u001b[0m attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[0;32m    391\u001b[0m     hidden_states,\n\u001b[0;32m    392\u001b[0m     layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[0;32m    393\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    394\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    395\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    396\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    397\u001b[0m )\n\u001b[0;32m    398\u001b[0m attn_output \u001b[39m=\u001b[39m attn_outputs[\u001b[39m0\u001b[39m]  \u001b[39m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[0;32m    399\u001b[0m outputs \u001b[39m=\u001b[39m attn_outputs[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[1;32mc:\\Users\\luttredn\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\luttredn\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:331\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    329\u001b[0m     attn_output, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n\u001b[0;32m    330\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m     attn_output, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_attn(query, key, value, attention_mask, head_mask)\n\u001b[0;32m    333\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_merge_heads(attn_output, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n\u001b[0;32m    334\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_proj(attn_output)\n",
      "File \u001b[1;32mc:\\Users\\luttredn\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:186\u001b[0m, in \u001b[0;36mGPT2Attention._attn\u001b[1;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[0;32m    183\u001b[0m attn_weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(query, key\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m))\n\u001b[0;32m    185\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale_attn_weights:\n\u001b[1;32m--> 186\u001b[0m     attn_weights \u001b[39m=\u001b[39m attn_weights \u001b[39m/\u001b[39m torch\u001b[39m.\u001b[39;49mfull(\n\u001b[0;32m    187\u001b[0m         [], value\u001b[39m.\u001b[39;49msize(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m) \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m \u001b[39m0.5\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mattn_weights\u001b[39m.\u001b[39;49mdtype, device\u001b[39m=\u001b[39;49mattn_weights\u001b[39m.\u001b[39;49mdevice\n\u001b[0;32m    188\u001b[0m     )\n\u001b[0;32m    190\u001b[0m \u001b[39m# Layer-wise attention scaling\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale_attn_by_inverse_layer_idx:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "saliency_scores = []\n",
    "for s in example['article']:\n",
    "    s.replace('\\n', '')\n",
    "    saliency_score = salience_score(s, test_doc['abstract'])\n",
    "    saliency_scores.append({\"sentence\": s, \"saliency_score\": saliency_score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max saliency score: -0.7890425486998125\n",
      "Min saliency score: -0.7942301034927368\n",
      "Average saliency score: -0.7916782622841124\n"
     ]
    }
   ],
   "source": [
    "max_saliency_score = max(saliency_scores, key=lambda x: x['saliency_score'])\n",
    "print(f\"Max saliency score: {max_saliency_score['saliency_score']}\")\n",
    "min_saliency_score = min(saliency_scores, key=lambda x: x['saliency_score'])\n",
    "print(f\"Min saliency score: {min_saliency_score['saliency_score']}\")\n",
    "avg_saliency_score = sum([s['saliency_score'] for s in saliency_scores])/len(saliency_scores)\n",
    "print(f\"Average saliency score: {avg_saliency_score}\")\n",
    "# print(saliency_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2e25a6a6007380c2aaab295dbd93e746300d3d483f6c80ab0a58bac2da9fb50d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
