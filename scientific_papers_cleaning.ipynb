{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import regex, nltk, json\n",
    "import pandas as pd\n",
    "import tensorflow_datasets as tfds\n",
    "import pydetex.pipelines as pi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and setup data\n",
    "This is going to be janky because tensorflow won't let you customize what features to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: This dataset is 12GB!!!\n",
    "# Download the dataset. You can't change the features that are loaded though so we need to load it a different way\n",
    "builder = tfds.builder('scientific_papers')\n",
    "builder.download_and_prepare()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luttredn\\tensorflow_datasets\\downloads\\extracted\\ZIP.ucid_1b3rmCSIoh6VhD4H-cSwcwbeC_export_downloadgu0w3Xxmpkl-6z18MJDCdOnjLAEkOPjguzzOPmwfyto\\arxiv-dataset\n"
     ]
    }
   ],
   "source": [
    "# Find the path to the downloaded arxiv dataset\n",
    "path = str(builder.data_path)+'\\\\..\\\\..\\\\..\\\\downloads\\\\extracted'\n",
    "path = os.path.abspath(path)\n",
    "for root, dirs, files in os.walk(path):\n",
    "    if 'arxiv-dataset' in dirs:\n",
    "        path = os.path.join(root, 'arxiv-dataset')\n",
    "        break\n",
    "else:\n",
    "    raise Exception('Could not find the arxiv dataset')\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(path, 'test.txt')\n",
    "features = [\"article_id\", \"article_text\", \"abstract_text\"]\n",
    "df = pd.read_json(data_path, lines=True)[features]\n",
    "df = df.rename(columns={\"article_text\": \"article\", \"abstract_text\": \"abstract\"})\n",
    "df = df.sample(n=500, random_state=1, ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>article</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gr-qc0101015</td>\n",
       "      <td>[there is considerable current interest in stu...</td>\n",
       "      <td>[&lt;S&gt; in this paper we consider the collision o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0803.1640</td>\n",
       "      <td>[the first data system requiring dark energy (...</td>\n",
       "      <td>[&lt;S&gt; upcoming weak lensing ( wl ) surveys can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1510.01821</td>\n",
       "      <td>[quantum key distribution ( qkd ) is the first...</td>\n",
       "      <td>[&lt;S&gt; the fully symmetric gaussian tripartite e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1105.2448</td>\n",
       "      <td>[the active galactic nucleus ( agn ) unificati...</td>\n",
       "      <td>[&lt;S&gt; x - ray unabsorbed seyfert 2 galaxies app...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1602.04433</td>\n",
       "      <td>[deep neural networks have significantly impro...</td>\n",
       "      <td>[&lt;S&gt; the recent success of deep neural network...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     article_id                                            article  \\\n",
       "0  gr-qc0101015  [there is considerable current interest in stu...   \n",
       "1     0803.1640  [the first data system requiring dark energy (...   \n",
       "2    1510.01821  [quantum key distribution ( qkd ) is the first...   \n",
       "3     1105.2448  [the active galactic nucleus ( agn ) unificati...   \n",
       "4    1602.04433  [deep neural networks have significantly impro...   \n",
       "\n",
       "                                            abstract  \n",
       "0  [<S> in this paper we consider the collision o...  \n",
       "1  [<S> upcoming weak lensing ( wl ) surveys can ...  \n",
       "2  [<S> the fully symmetric gaussian tripartite e...  \n",
       "3  [<S> x - ray unabsorbed seyfert 2 galaxies app...  \n",
       "4  [<S> the recent success of deep neural network...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 376\n",
      "id: hep-th0204258\n"
     ]
    }
   ],
   "source": [
    "# Pick a specific or random example\n",
    "# example = df.iloc[442]\n",
    "example = df.sample(n=1).iloc[0]\n",
    "print(f\"idx: {example.name}\")\n",
    "print(f\"id: {example['article_id']}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Not LaTeX stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<S> a systematic approach to the description of gauge invariant charges is presented and applied to the construction of both the static colour charge configuration in qcd and the monopole solution in pure su(2 ) . </S>',\n",
       " '<S> the gauge invariant non - abelian monopole offers a new style of order parameter for monopole condensation . </S>']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[\"abstract\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it is a fact of life , and one that has been exploited again and again at this meeting , that even gauge invariant objects are more transparent in a specific gauge .',\n",
       " 'for example , the static interquark potential , which has a well known gauge invariant definition , is often best treated in coulomb gauge ( see , for example , @xcite and more recently  @xcite ) as this gauge is , in some way , closely adapted to that physical system .',\n",
       " 'usually such a choice of gauge is a simple pragmatic decision based on the simplicity of the resulting calculation .',\n",
       " 'however , what we d like to argue is that the connection between such an adapted gauge and the correct gauge invariant description , often goes much deeper  @xcite .',\n",
       " 'we will see that such appropriate gauge fixings , once recognised , can lead to an understanding of the dominant contribution to the gauge invariant description of the relevant physical degrees of freedom . for monopoles and vortices , where gauge invariant formulation do not yet exist , understanding this route from gauge fixing to gauge invariance']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[\"article\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convience function to print a set\n",
    "def print_set(s, joiner=' ', sort=True):\n",
    "    if sort:\n",
    "        s = sorted(s)\n",
    "    if joiner is None:\n",
    "        return s\n",
    "    if len(s)!=0 and s[0] is not str:\n",
    "        s = [str(x) for x in s]\n",
    "    return joiner.join(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Characters: $ ( ) , - . / : < = > @ [ \\ ] _\n",
      "Abstract Exclusive Characters: / < >\n",
      "Article Exclusive Characters: $ ( ) , - . : = @ [ \\ ] _\n"
     ]
    }
   ],
   "source": [
    "# See what kind of non alpha numeric characters are left over\n",
    "abstract_chars = set(re.findall(r\"[^a-zA-Z0-9 ]\", ' '.join(example[\"abstract\"])))\n",
    "article_char = set(re.findall(r\"[^a-zA-Z0-9 ]\", ' '.join(example[\"article\"])))\n",
    "total_chars = abstract_chars.union(article_char)\n",
    "abstract_chars = total_chars - article_char\n",
    "article_char = total_chars - abstract_chars\n",
    "print(f\"All Characters: {print_set(total_chars)}\")\n",
    "print(f\"Abstract Exclusive Characters: {print_set(abstract_chars)}\")\n",
    "print(f\"Article Exclusive Characters: {print_set(article_char)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Tags: </S> <S>\n",
      "Abstract Exclusive Tags: </S> <S>\n",
      "Article Exclusive Tags: \n"
     ]
    }
   ],
   "source": [
    "# Dataset used tags for various things. Check if there are any tags still left\n",
    "abstract_tags = set(re.findall(r\"<[/]?\\w+[/]?>\", ' '.join(example[\"abstract\"])))\n",
    "article_tags = set(re.findall(r\"<[/]?\\w+[/]?>\", ' '.join(example[\"article\"])))\n",
    "total_tags = abstract_tags.union(article_tags)\n",
    "abstract_tags = total_tags - article_tags\n",
    "article_tags = total_tags - abstract_tags\n",
    "print(f\"All Tags: {print_set(total_tags)}\")\n",
    "print(f\"Abstract Exclusive Tags: {print_set(abstract_tags)}\")\n",
    "print(f\"Article Exclusive Tags: {print_set(article_tags)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract Indices with multiple sentences: 2/2\n",
      "\t(2, 2)\n",
      "Article Indices with multiple sentences: 28/81\n",
      "\t(1, 53) (2, 27) (3, 1)\n"
     ]
    }
   ],
   "source": [
    "# Are there any indices with multiple sentences?\n",
    "abstract_sent_counts = [len(nltk.tokenize.sent_tokenize(sent)) for sent in example[\"abstract\"]]\n",
    "article_sent_counts = [len(nltk.tokenize.sent_tokenize(sent)) for sent in example[\"article\"]]\n",
    "\n",
    "print(f\"Abstract Indices with multiple sentences: {  sum([1 for count in abstract_sent_counts if count > 1])  }/{  len(example.abstract)  }\")\n",
    "abstract_sent_counts = [(i, abstract_sent_counts.count(i)) for i in set(abstract_sent_counts)]\n",
    "print(\"\\t\" + print_set(abstract_sent_counts))\n",
    "\n",
    "print(f\"Article Indices with multiple sentences: {  sum([1 for count in article_sent_counts if count > 1])  }/{  len(example.article)  }\")\n",
    "article_sent_counts = [(i, article_sent_counts.count(i)) for i in set(article_sent_counts)]\n",
    "print(\"\\t\" + print_set(article_sent_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( [ abeliang ] ) ( [ ced ] ) ( [ df ] ) ( [ dm ] ) ( [ etp ] ) ( [ gid ] ) ( [ qedmin ] ) ( [ r1b ] ) ( [ sd ] ) ( [ su2 dm ] )\n"
     ]
    }
   ],
   "source": [
    "# The document also has some weird tags\n",
    "weird_tags = set(re.findall(r\"\\( \\[ [^\\(\\[\\)\\]]+ \\] \\)\", ' '.join(example[\"article\"])))\n",
    "print(print_set(weird_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@xcite @xmath\n"
     ]
    }
   ],
   "source": [
    "# There are these other weird tags, but I think I'm going to try keeping them\n",
    "weird_tags = set(re.findall(r\"@x[a-z]+\", ' '.join(example[\"article\"])))\n",
    "print(print_set(weird_tags))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figuring out how to deal with LaTex\n",
    "The dataset has most latex parsed out but there is still a decent amount remaining... and due to the poor attempt made previously, it's really hard to parse out the stragglers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing weird tags for now\n",
    "example_article = ' '.join(example.article)\n",
    "example_article = re.sub(r\"\\( \\[ [^\\(\\[\\)\\]]+ \\] \\)\", \"\", example_article)\n",
    "example_article = re.sub(r\"\\n\", \"\", example_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lence of the construction . ] and its equal - time commutator with the potential is @xmath16=f_i(x)m\\,.\\ ] ] so we can interpret @xmath17 as a monopole creation operator for the pure abelian theory . alth\n"
     ]
    }
   ],
   "source": [
    "# Find groups of LaTeX (if any \\ is within 100 characters of another \\ then it is a group)\n",
    "math_groups = [x.group() for x in re.finditer(r\"([^\\\\]{0,100}\\\\[^\\\\]{0,100})+\", example_article)]\n",
    "print(\"\\n\\n\".join(math_groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lence of the construction . ] and its equal - time commutator with the potential is @xmath16=f_i(x)m so we can interpret @xmath17 as a monopole creation operator for the pure abelian theory . alth\n"
     ]
    }
   ],
   "source": [
    "# A common pattern I want to remove is \\blah \\blah blah ] ]\n",
    "math_groups = [regex.sub(r\"(\\\\((?>[^\\\\\\]]+|(?R))*+)])\", \"\", math_group) for math_group in math_groups]\n",
    "print(\"\\n\\n\".join(math_groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lence of the construction . ] and its equal - time commutator with the potential is @xmath16=f_i(x)m so we can interpret @xmath17 as a monopole creation operator for the pure abelian theory . alth\n"
     ]
    }
   ],
   "source": [
    "# Try removing the latex expressions (this is really slow and annoying to work with after)\n",
    "# latex_removed = [pi.strict(sent) for sent in math_groups]\n",
    "\n",
    "# Alternatively, we can try removing words that start with a backslash and have curly brace groups afterwards\n",
    "latex_removed = [regex.sub(r\"((?>\\\\[^\\s{]*+)({(?>[^{}]+|(?2))*+})*)\", \"\", sent) for sent in math_groups]\n",
    "\n",
    "print(\"\\n\\n\".join(latex_removed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lence of the construction . and its equal time commutator with the potential is @xmath16 so we can interpret @xmath17 as a monopole creation operator for the pure abelian theory . alth\n"
     ]
    }
   ],
   "source": [
    "# Remove the groups of non standard characters\n",
    "reg_exp = r\"((?![a-zA-Z] )(?:[a-zA-Z ]{0,2}[^a-zA-Z0-9.,;:@\\s][a-zA-Z ]{0,2})+(?<! [a-zA-Z]))\"\n",
    "special_chars_removed = [re.sub(reg_exp, \" \", sent) for sent in latex_removed]\n",
    "print(\"\\n\\n\".join(special_chars_removed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lence of the construction . and its equal time commutator with the potential is @xmath16 so we can interpret @xmath17 as a monopole creation operator for the pure abelian theory . alth\n"
     ]
    }
   ],
   "source": [
    "compress_spaces = [re.sub(r\"\\s+\", \" \", sent) for sent in special_chars_removed]\n",
    "print(\"\\n\\n\".join(compress_spaces))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a pain in the ass but I think it works pretty well"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The whole latex removal pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is a fact of life , and one that has been exploited again and again at this meeting , that even gauge invariant objects are more transparent in a specific gauge .\n",
      "for example , the static interquark potential , which has a well known gauge invariant definition , is often best treated in coulomb gauge see , for example , @xcite and more recently @xcite as this gauge is , in some way , closely adapted to that physical system .\n",
      "usually such a choice of gauge is a simple pragmatic decision based on the simplicity of the resulting calculation .\n",
      "however , what we d like to argue is that the connection between such an adapted gauge and the correct gauge invariant description , often goes much deeper @xcite .\n",
      "we will see that such appropriate gauge fixings , once recognised , can lead to an understanding of the dominant contribution to the gauge invariant description of the relevant physical degrees of freedom .\n",
      "for monopoles and vortices , where gauge invariant formulation do not yet exist , understanding this route from gauge fixing to gauge invariance is of central importance if we are ever to understand fully their roles in the non perturbative structure of qcd .\n",
      "in order to map out the connection between static quarks and the coulomb gauge , we need to make precise just what we mean by saying that a field @xmath0 describes a static quark .\n",
      "first , to capture the requirement that quarks carry colour , the field @xmath0 can not be a coloured singlet under the global gauge transformation although it must be gauge invariant under the local gauge transformations .\n",
      "this tells us straight away that it can not be identified with the matter field @xmath1 that enters directly into the formulation of the theory since under a gauge transformation we have @xmath2 .\n",
      "what we must have is that @xmath3 , for some field dependent configuration @xmath4 , where under a gauge transformation we have @xmath5 we call @xmath4 a dressing for the charge .\n",
      "it incorporates the cloud of fields around any charge .\n",
      "to impose the static condition that @xmath6 , we need to realise that static means infinite mass and hence we can exploit the dynamical simplification that comes from the heavy quark effective theory .\n",
      "that is , we can use the equations of motion that the matter field is covariantly constant : @xmath7 .\n",
      "using this , and the condition that @xmath0 is static , it is easy to see that the dressing must satisfy the static dressing equation @xmath8 equations and are the fundamental conditions the dressing must satisfy in order to construct a static charge .\n",
      "explicit solutions to them can be found in qed @xcite , and perturbative solutions to them can be constructed in qcd @xcite .\n",
      "the point to note here is that the resulting dressing has structu .\n",
      "the charged field factorises into the product of two separately gauge invariant terms @xmath9 the bracketed term involves an anti time ordering while the rest of the expression is local in time but non local in space .\n",
      "we call @xmath10 the minimal part of the dressing as it is essential for the overall gauge invariance of the charge .\n",
      "additional terms , such as @xmath11 in , are not expected from the overall requirement of gauge invariance .\n",
      "rather , they are needed to ensure the correct dynamical properties of the charge .\n",
      "the significance of this factorisation can be seen in either the infra red properties of the fields or in the forces between two such charges .\n",
      "it emerges @xcite that in qed the minimal part of the dressing is responsible for controlling the soft infra red structure of the theory , while the additional part deals with the phase divergences .\n",
      "in terms of forces @xcite , the minimal part gives the anti screening contribution to the inter quark potential , while the additional term is needed for the lesser screening forces .\n",
      "given the dominance of anti screening over screening , we see that the minimal part of the dressing is capturing the dominant glue content of a static charge .\n",
      "there are many important and interesting properties of these fields , but the key thing to note here is that the minimal part of the dressing becomes the identity in coulomb gauge .\n",
      "this important fact is most easily seen in qed where @xmath12 and @xmath13 is the classical coulombic electric field of a static charge .\n",
      "we see that coulomb gauge is the unique gauge that trivialises the dominant part of the static dressing .\n",
      "this now makes precise the sense in which the coulomb gauge is adapted to the description of static charges .\n",
      "knowing this connection can , in turn , provide an efficient means for calculating the minimal part of the dressing which is , as we ve seen , essential for a gauge invariant description of the static charge .\n",
      "this connection between gauge invariance and gauge fixing can be generalised to moving charges @xcite and it can also be given a simple geometric and hence global interpretation @xcite .\n",
      "the conclusion from such an analysis is that in qcd there is a global obstruction to the construction of a static coloured charge , and that this is how confinement is seen in this approach .\n",
      "we now want to consider a pure non abelian gauge theory and show how a monopole creation operator can be constructed .\n",
      "the are several reasons for wanting to do this , the most immediate being to allow for the construction of new order parameters with which the dual superconductor account of confinement can be tested .\n",
      "it is also , as we ll see , an interesting theoretical study of the relation between classical solutions and quantum configurations in gauge theories .\n",
      "to motivate our approach , we note that in dirac s original account of the construction of electric charges @xcite , he arrived at the minimal , abelian , static dressing by noting that its commutator with the electric field operator generated the coulombic electric field expected from a static charge .\n",
      "as we will see , a similar argument can be applied to monopoles .\n",
      "let us start again in the abelian theory .\n",
      "suppose that @xmath14 is the classical dirac monopole potential @xcite .\n",
      "then it is straightforward to see that the operator @xmath15 is gauge invariant will yield different , but weakly equivalent monopole operators .\n",
      "thus the ability to move the dirac string is seen in the weak equivalence of the construction .\n",
      "and its equal time commutator with the potential is @xmath16 so we can interpret @xmath17 as a monopole creation operator for the pure abelian theory .\n",
      "although this operator allows us to rederive many of the important properties of monopoles , the singularity of the potential @xmath18 makes this an artificial construction in qed .\n",
      "this reflects the fact that we do not expect monopoles in such an abelian theory .\n",
      "however , in non abelian theories regular monopole solutions are known to exist when spontaneous symmetry breaking occurs , and they are conjectured to exist and to play an important role in pure gauge theories see for example , ref @xcite .\n",
      "with this in mind , we now investigate how can be generalised to the non abelian theory .\n",
      "the naive extension of this simple construction to a non abelian theory , where we replace the abelian potential @xmath18 by a non abelian one @xmath19 and the electric field @xmath20 by its chromo electric generalisation @xmath21 , runs into two immediate problems .\n",
      "the first is to decide on how to generalise the classical dirac monopole configuration .\n",
      "the second is maintaining gauge invariance since , as is well known , the chromo electric field is not gauge invariant .\n",
      "if we now specialise to a pure 2 gauge theory , then there is a natural candidate for a monopole configuration first written down by wu and yang @xcite : @xmath22 this is a solution to the classical yang mills equations of motion which , through a singular gauge transformation , can be related to the abelian monopole configuration .\n",
      "it should be noted , though , that it is an unstable solution @xcite .\n",
      "how this is modified or reflected in the quantum theory is , however , unknown .\n",
      "the gauge non invariance of the chromo electric field seems a much more serious obstacle to the construction of a non abelian generalisation of .\n",
      "extending the dressing technique used in the description of a static charge , we will solve this problem by dressing the chromo electric field @xmath23 where the dressing transforms as in so that @xmath24 is now gauge invariant .\n",
      "the monopole creation operator generalising is then @xmath25 given that we want to describe a static monopole , i.e.\n",
      ", it should not generate a chromo electric field , we require @xmath26 0 .\n",
      ", the dressing must solely depend on the chromo electric field .\n",
      "as such , we can not simply use the dressing constructed in to describe a static quark .\n",
      "to proceed , we recall the close relation between monopoles and abelian gauge fixing @xcite .\n",
      "following our method for describing the minimally dressed static quark , we will exploit this adapted class of gauge fixings to construct a gauge invariant chromo electric field and hence the dressing needed in .\n",
      "gauge fixing in the chromo electric sector is , as far as we know , not well studied .\n",
      "the interesting point here is that it is not possible to fully fix the gauge .\n",
      "in terms of constraints , one can easily see that it is impossible to construct a complete second class set out of gauss law and functions of just the chromo electric field .\n",
      "however , second class subsets can be found that are valid on regions of the phase space .\n",
      "to see how this works , consider the simple chromo electric gauge @xmath28 these , along with the components @xmath29 and @xmath30 of gauss law , form a second class set of constraints as long as @xmath31 .\n",
      "to implement this reduction then , we should restrict ourselves to the regions in phase space where either @xmath32 or @xmath33 .\n",
      "if @xmath34 , then we can either take it and one of the components in as our gauge , or we can choose another gauge by looking at , say , the @xmath35 components of the chromo electric field .\n",
      "in this way , through a patching process , we can implement a chromo electric gauge fixing that is only ill defined on configurations which have zero field strength .\n",
      "we do not yet fully understand the effect of such instanton configurations on our monopole construction , so for the moment we will neglect them and , for simplicity , just consider the gauge in the region @xmath32 .\n",
      "having settled on a gauge that we know is adapted , or at least sympathetic , to the non abelian monopole configuration , we now have to find the dressing needed in by rotating our fields into the gauge fixed configuration .\n",
      "for a configuration space gauge fixing , such as the coulomb gauge , we were guaranteed that the resulting dressing would at least locally satisfy the fundamental relation .\n",
      "the incompleteness of the chromo electric gauge fixing , though , means that a little more work is needed to get the correct transformation properties of the dressing .\n",
      "in terms of the dressed fields , we need to solve the equations @xmath36 .\n",
      "now @xmath37 where @xmath38 is a rotation matrix .\n",
      "hence we wish to solve @xmath39 and @xmath40 .\n",
      "these two equations are simple vector equations and can be immediately solved as follows .\n",
      "take @xmath41 where @xmath42 and @xmath43 is , for the moment , an arbitrary unit vector .\n",
      "then @xmath44 and @xmath45 these allow us to construct the rotation matrix and check gauge invariance of the resulting dressed chromo electric field .\n",
      "for the third colour component gauge invariance is immediate since @xmath46 we further note that @xmath47 , which with our restriction that @xmath32 is just @xmath48 in the gauge .\n",
      "for the other components of the dressed chromo electric field , though , gauge invariance can only be ensured through a good choice of @xmath49 .\n",
      "from the definition we have @xmath50 and @xmath51 gauge invariance will follow if @xmath52 is proportional to @xmath53 .\n",
      "however , from , we also need @xmath49 orthogonal to @xmath54 .\n",
      "there are various ways to satisfy these conditions for gauge invariance .\n",
      "for example , we could take @xmath55 in summary , we have seen in this section how to generalise the abelian monopole creation operator to yield a gauge invariant monopole operator .\n",
      "this was done by construction a chromo electric dressing adapted to the chromo electric gauge fixing .\n",
      "an important contribution to the dressing approach to gauge invariance is the recognition that there are special adapted gauges that have a particular significance for the description of both chromo electric and magnetic charges in non abelian gauge theories .\n",
      "for electric charges in both qed and qcd , these adapted gauges followed naturally from a more fundamental dressing equation .\n",
      "solving that equation factorises the dressing into a dominant anti screening term that controlled the soft infra red sector and an additional screening term .\n",
      "for a specific dynamical configuration for the charges , the adapted gauge trivialises the dominant part of the dressing .\n",
      "however , it should be stressed that in a scattering situation where charges with differing momentum must be dressed differently , there is no gauge in which all the different dressings so simplify .\n",
      "in pure 2 theory we have seen how to go from chromo electric gauge fixing to a gauge invariant monopole creation operator .\n",
      "as yet there is no analogous dynamical approach to this dressing .\n",
      "it is hoped , though , that through our recognition of the adapted gauge to this system we have also captured the dominant monopole configuration .\n",
      "this will allow us to further probe the role of monopoles in confinement .\n"
     ]
    }
   ],
   "source": [
    "# Removing weird tags for now\n",
    "example_article = ' '.join(example.article)\n",
    "example_article = re.sub(r\"\\( \\[ [^\\(\\[\\)\\]]+ \\] \\)\", \"\", example_article)\n",
    "example_article = re.sub(r\"\\n\", \"\", example_article)\n",
    "\n",
    "# REMOVING LATEX\n",
    "# Remove \\blah \\blah blah ] ]\n",
    "example_article = regex.sub(r\"(\\\\((?>[^\\\\\\]]+|(?R))*+)])\", \"\", example_article)\n",
    "\n",
    "# Remove words that start with a backslash and have curly brace groups\n",
    "example_article = regex.sub(r\"((?>\\\\[^\\s{]*+)({(?>[^{}]+|(?2))*+})*)\", \"\", example_article)\n",
    "\n",
    "# Remove the groups of non standard characters\n",
    "reg_exp = r\"((?![a-zA-Z] )(?:[a-zA-Z ]{0,2}[^a-zA-Z0-9.,:@\\s][a-zA-Z ]{0,2})+(?<! [a-zA-Z]))\"\n",
    "example_article = re.sub(reg_exp, \" \", example_article)\n",
    "\n",
    "# Compress whitespace\n",
    "example_article = re.sub(r\"\\s+\", \" \", example_article)\n",
    "\n",
    "# Sentence tokenize\n",
    "example_article = nltk.tokenize.sent_tokenize(example_article)\n",
    "\n",
    "# Remove sentences that are too short (less than 5 words, excluding punctuation and numbers)\n",
    "example_article = list(filter(lambda x: len(list(filter(lambda x: x.isalpha(), x.split()))) > 5, example_article))\n",
    "\n",
    "print('\\n'.join(example_article))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_latex(doc):\n",
    "    # Remove \\blah \\blah blah ] ]\n",
    "    doc = regex.sub(r\"(\\\\((?>[^\\\\\\]]+|(?R))*+)])\", \"\", doc)\n",
    "\n",
    "    # Remove words that start with a backslash and have curly brace groups\n",
    "    doc = regex.sub(r\"((?>\\\\[^\\s{]*+)({(?>[^{}]+|(?2))*+})*)\", \"\", doc)\n",
    "\n",
    "    # Remove the groups of non standard characters\n",
    "    reg_exp = r\"((?![a-zA-Z] )(?:[a-zA-Z ]{0,2}[^a-zA-Z0-9.,:@\\s][a-zA-Z ]{0,2})+(?<! [a-zA-Z]))\"\n",
    "    doc = re.sub(reg_exp, \" \", doc)\n",
    "\n",
    "    # Compress whitespace\n",
    "    doc = re.sub(r\"\\s+\", \" \", doc)\n",
    "    return doc\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # undo bad sent tokenization\n",
    "    doc = ' '.join(doc)\n",
    "\n",
    "    # remove \\n\n",
    "    doc = re.sub(r'\\n', ' ', doc)\n",
    "\n",
    "    # remove sentence tags from abstract\n",
    "    doc = re.sub(r'<S>|</S>', '', doc)\n",
    "\n",
    "    # remove weird tags: ( [ blah ] )\n",
    "    doc = re.sub(r\"\\( \\[ [^\\(\\[\\)\\]]+ \\] \\)\", \"\", doc)\n",
    "\n",
    "    # remove latex\n",
    "    doc = remove_latex(doc)\n",
    "\n",
    "    # Sentence tokenize\n",
    "    doc = nltk.tokenize.sent_tokenize(doc)\n",
    "\n",
    "    # Strip each sentence\n",
    "    doc = [sent.strip() for sent in doc]\n",
    "\n",
    "    # Remove sentences that are too short (less than 5 words, excluding punctuation and numbers)\n",
    "    doc = list(filter(lambda x: len(list(filter(lambda x: x.isalpha(), x.split()))) > 5, doc))\n",
    "    return doc\n",
    "\n",
    "def normalize_corpus(corpus):\n",
    "    return corpus.map(lambda x: normalize_document(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing article text...\n",
      "Normalizing abstract text...\n"
     ]
    }
   ],
   "source": [
    "cleaned_df = pd.DataFrame(columns=[\"article_id\", \"article\", \"abstract\"])\n",
    "cleaned_df[\"article_id\"] = df[\"article_id\"]\n",
    "print(\"Normalizing article text...\")\n",
    "cleaned_df[\"article\"] = normalize_corpus(df[\"article\"])\n",
    "print(\"Normalizing abstract text...\")\n",
    "cleaned_df[\"abstract\"] = normalize_corpus(df[\"abstract\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>article</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gr-qc0101015</td>\n",
       "      <td>[there is considerable current interest in stu...</td>\n",
       "      <td>[in this paper we consider the collision of sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0803.1640</td>\n",
       "      <td>[the first data system requiring dark energy c...</td>\n",
       "      <td>[upcoming weak lensing surveys can be used to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1510.01821</td>\n",
       "      <td>[quantum key distribution qkd is the first mat...</td>\n",
       "      <td>[the fully symmetric gaussian tripartite entan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1105.2448</td>\n",
       "      <td>[the active galactic nucleus agn unification s...</td>\n",
       "      <td>[x ray unabsorbed seyfert 2 galaxies appear to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1602.04433</td>\n",
       "      <td>[deep neural networks have significantly impro...</td>\n",
       "      <td>[the recent success of deep neural networks re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     article_id                                            article  \\\n",
       "0  gr-qc0101015  [there is considerable current interest in stu...   \n",
       "1     0803.1640  [the first data system requiring dark energy c...   \n",
       "2    1510.01821  [quantum key distribution qkd is the first mat...   \n",
       "3     1105.2448  [the active galactic nucleus agn unification s...   \n",
       "4    1602.04433  [deep neural networks have significantly impro...   \n",
       "\n",
       "                                            abstract  \n",
       "0  [in this paper we consider the collision of sp...  \n",
       "1  [upcoming weak lensing surveys can be used to ...  \n",
       "2  [the fully symmetric gaussian tripartite entan...  \n",
       "3  [x ray unabsorbed seyfert 2 galaxies appear to...  \n",
       "4  [the recent success of deep neural networks re...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 53\n",
      "id: 1702.06415\n"
     ]
    }
   ],
   "source": [
    "# Pick a specific or random example\n",
    "# example = cleaned_df.iloc[442]\n",
    "example = cleaned_df.sample(n=1).iloc[0]\n",
    "print(f\"idx: {example.name}\")\n",
    "print(f\"id: {example['article_id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using the experimental data from the alice program on the centrality dependence of the transverse momentum @xmath0 spectra in collisions at @xmath1 tev , we show that the double tsallis distribution and the generalized fokker plank solution can not describe the spectra of pions , kaons and protons from central to peripheral collisions in the entire @xmath0 region , simultaneously .\n",
      "hence , a new two component distribution , which is a hydrodynamic extension of the generalized fp solution accounting for the collective motion effect in heavy ion collisions , is proposed in order to reproduce all the particle spectra .\n",
      "our results suggest that the particle production dynamics may be different for different particles , especially at very low @xmath0 region .\n"
     ]
    }
   ],
   "source": [
    "# The abstract\n",
    "print('\\n'.join(example.abstract))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the advent of a new generation of high energy collider experiments , such as relativistic heavy ion collider rhic and large hadron collider lhc , has launched a new era in the study of the hadron production .\n",
      "plenty of pp , pa and aa collisions data have been accumulated , which allow us to study the nature of the final particle production .\n",
      "the transverse momentum spectra carry important information about the dynamics of particle production and evolution process of interacting system formed in high energy nuclear collisions .\n",
      "in the past decade , the attempt to understand the particle production mechanism by different theoretical and phenomenological approaches has been a great success @xcite .\n",
      "generally , the theoretical investigation of hadron production in heavy ion collisions is operated into different camps , characterized by the regions of transverse momenta @xmath0 of the produced hadrons .\n",
      "at low @xmath0 statistical hadronization and hydrodynamical models are generally adopted @xcite , whereas at high @xmath0 jet production and parton fragmentation with suitable consideration of medium effects in perturbative qcd pqcd are the central themes @xcite .\n",
      "the approaches have been studied essentially independent of each other with credible success in interpreting the experimental data for different @xmath0 regions , since their dynamics are decoupled .\n",
      "at intermediate and lower @xmath0 recombination or coalescence subprocess reco in heavy ion collisions has been found to be more relevant , which has successfully explained various experimental data @xcite .\n",
      "beside the reco model , there are also other phenomenological models proposed to describe the hadron production .\n",
      "@xcite , a two component model , the particle spectra could be treated as a summation of an exponential boltzmann like and a power law distributions , was suggested .\n",
      "but this model could not describe the charged particle production at very high @xmath0 in central collisions at 2.76 tev .\n",
      "so an additional power law term was added , which was explained by the peculiar shape of the nuclear modification factor @xmath2 .\n",
      "on the other hand , due to the effect of the collective motion in large colliding system , the relativistic hydrodynamics is usually adopted to consider the particle production , instead of thermodynamic methods .\n",
      "therefore , the charged particle spectra could be consisted of a hydrodynamic term and two power law terms suggested as well in ref .\n",
      "@xcite , @xmath3 where @xmath4 is the transverse flow rapidity and the radial flow velocity is parametrized as @xmath5 with @xmath6 for the surface velocity .\n",
      "@xmath7 is a parameter related to the transverse size of the particle distribution in space and @xmath8 is the distance of the particle from the origin of the coordinate system in the transverse plane .\n",
      "@xmath9 is the transverse mass and @xmath10 is the rest mass of particle .\n",
      "@xmath11 and @xmath12 are the modified bessel functions .\n",
      "hence , there are eight free parameters @xmath13 , @xmath14 , @xmath15 , @xmath16 , @xmath17 , @xmath18 , @xmath19 , @xmath20 , which add the difficulty to fit , even it can describe the charged particle spectra very well in central collsions .\n",
      "other forms of multicomponent models , which were derived from multisource thermal model @xcite , were also applied to the particle transverse momentum spectra produced from low energy to high energy heavy ion collisions .\n",
      "besides , the tsallis distribution , which was proposed about three decades ago @xcite , has been widely applied to describe final particle production with great success by the theorists and experimentalists @xcite .\n",
      "it was derived in the framework of non extensive thermodynamics , @xmath21 and fokker , we show our results of particle spectra from collisions by a double tsallis distribution as well as the generalized fokker plank solution , respectively .\n",
      "a new two component distribution which is consisted of the hydrodynamic term and generalized fokker plank solution is proposed in section hydro .\n",
      "in section results , a detailed comparison among the three distributions is shown .\n",
      "finally , a summary is given in section summary .\n",
      "in our earlier work @xcite , it has been demonstrated that a single tsallis distribution could not fully reproduce the whole structure of the observed particle spectra in central collisions at @xmath1 tev .\n",
      "therefore , a double tsallis distribution could be proposed , @xmath25 where @xmath26 is the transverse energy .\n",
      "when the rest mass of particle @xmath27 , becomes the same as the double tsallis distribution proposed in ref .\n",
      "@xcite for charged particles , which are dominated by pions .\n",
      "but when @xmath10 is large , such as kaons , protons and antiprotons , etc .\n",
      ", the mass effect should be taken into account .\n",
      "compared with the single tsallis distribution , three more parameters are increased , which allowed one to fit the charged particle spectra with @xmath0 up to 100 gev c in the most central collisions @xcite .\n",
      "recently , the transverse momentum spectra of charged pions , kaons and protons up to @xmath28 gev c have been measured in collisions at @xmath1 tev using the alice detector for six different centrality classes @xcite .\n",
      "hence , it is right time for us to investigate whether the double tsallis distribution can describe the identified particle production both at central and non central collisions .\n",
      "as shown in figure tsallis , reproduces the data for pions and kaons very well , while for protons the situation is different .\n",
      "firstly , it is remarkable that the spectrum of protons at the centrality 60 80 could be described by very well .\n",
      "this is reasonable and agrees with our previous work .\n",
      "the peripheral collisions in aa are more similar to the collisions and the single tsallis distribution can fit all the particle spectra produced in collisions @xcite .\n",
      "secondly , we also have to notice that in figure tsall when @xmath29 gev c , the magenta solid lines are much larger than the data for the central and less central collisions .\n",
      "the existence of difference is not surprising .\n",
      "compared with pions and kaons , the spectra of protons demonstrate different behaviors at low @xmath0 .\n",
      "the particle production dynamics may be different for different particles .\n",
      "on the one hand , a cascade particle production mechanism was proposed in collisions .\n",
      "the heavier particles are more likely to be produced at the beginning while the light particles can be produced at all times @xcite .\n",
      "on the other hand , in the quark recombination models @xcite , mesons are formed by combining a quark and an antiquark while baryons by three quarks .\n",
      "because different numbers of an arks participate in forming the particles , the structures of their spectra must be different .\n",
      "in this sense , our investigation results urge more studies on particle production mechanisms .\n",
      "co , , , generally speaking , the results shown in the previous sections indicate that the three different methods , which are the double tsallis distribution , the generalized fokker plank solution and the new two component distribution , can describe the experimental data .\n",
      "but we want to understand which distribution is the optimal choice .\n",
      "to have a clearer picture , we evaluate the degree of agreement of the fitted results with the experimental data .\n",
      "one can calculate the ratio between the experimental data and the fitted results , which is defined as @xmath30 figure ratio1 shows the ratio @xmath7 , calculated by , and respectively , as a function of the transverse momentum @xmath0 in linear scale for the centrality 0 5 .\n",
      "in figure ratio1 , one can see that all points for pions produced in most central collisions from are in the range from 0.1 to 0.1 , while the relative discrepancies from and are large at low @xmath0 region .\n",
      "for kaons , and have the similar fitting power as shown in figure ratio1 , the deviation of the fitting results from data is less than 10 .\n",
      "while , for protons , figure ratio1 establishes that is not good for low @xmath0 region , which can be easily seen from figure tsall .\n",
      "for the sake of the comprehensive comparison , we should also check the relative discrepancies of the three equations at other centralities .\n",
      "here , we only plot the results for the centrality 60 80 .\n",
      "except a few points , the relative discrepancies of the three equations for pions , kaons and protons are in good agreement with the data with deviation from the data less than 10 .\n",
      "remarkably , the fluctuations for pions , kaons , and protons are much smaller than those for the centrality 0 5 .\n",
      "in other words , the three distributions agree with each other to describe better the particle spectra produced in peripheral collisions , which are similar to collisions .\n",
      "based on these analyses , we can conclude that is the best one among the three distributions , which is composed of a hydrodynamic term and the generalized fokker plank solution .\n",
      "it could well describe the spectra from central to peripheral collisions for pions , kaons and protons .\n",
      "the proposed hydrodynamic extension of slightly modifies the description of the experimental data for pions at low @xmath0 region , which also gives insight to the particle production mechanism .\n",
      "in this paper , we have made a detailed study of the double tsallis distribution , the generalized fokker plank solution and the new two component distribution , by fitting the transverse momentum spectra of pions , kaons and protons in collisions at @xmath31 tev .\n",
      "the double tsallis distribution can fit the particle spectra except the big deviation observed for proton at @xmath32 gev c for central and less central collisions , while the generalized fokker plank solution is not able to describe the spectra of pions at very low @xmath0 .\n",
      "therefore , we propose a new two component distribution as a hydrodynamic extension of the generalized fokker plank solution accounting for the collective motion effect in order to fit all the particle spectra in collisions , especially for extremely low @xmath0 region .\n",
      "according to these results , we can conclude that the new two component distribution is the optimal method .\n",
      "from these analyses , we get more information about the particle production mechanism in collision .\n",
      "we also wish more exciting results could be found in collisions at @xmath33 tev .\n",
      "the authors declare that there is no conflict of interests regarding the publication of this article .\n",
      "this work is supported by the nsfc of china under grant no .\n",
      "p. braun munzinger , k. redlich and j. stachel , particle production in heavy ion collisions , quark gluon plasma 3 , edited by r. c. hwa and x .\n",
      "p. f. kolb and u. heinz , hydrodynamic description of ultrarelativistic heavy ion collisions , quark gluon plasma 3 , edited by r. c. hwa and x .\n",
      "c. gale , s. jeon and b. schenke , hydrodynamic modeling of heavy ion collisions , int .\n",
      "r. baier , y. l. dokshitzer , a. h. mueller , s. peigne and d. schiff , radiative energy loss and broadening of high energy partons in nuclei , nucl .\n",
      "x. f. guo and x. n. wang , multiple scattering , parton energy loss and modified fragmentation functions in deeply inelastic e a scattering , phys .\n",
      "jet collaboration , extracting the jet transport coefficient from jet quenching in high energy heavy ion collisions , phys .\n",
      "v. greco , c. m. ko and p. levai , parton coalescence and anti proton pion anomaly at rhic , phys .\n",
      "90 , 202302 2003 r. j. fries , b. mller , c. nonaka , and s. a. bass , hadronization in heavy ion collisions : recombination and fragmentation of partons , phys .\n",
      "r. c. hwa and c. b. yang , recombination of shower partons at high @xmath0 in heavy ion collisions , phys .\n",
      "a. a. bylinkin , n. s. chernyavskaya and a. a. rostovtsev , hydrodynamic extension of a two component model for hadroproduction in heavy ion collisions , phys .\n",
      "a. a. bylinkin , n. s. chernyavskaya and a. a. rostovtsev , two components in charged particle production in heavy ion collisions , nucl .\n",
      "h. r. wei , y. h. chen , l. n. gao and f. h. liu , comparing multicomponent erlang distribution and levy distribution of particle transverse momentums , adv .\n",
      "f. h. liu , unified description of multiplicity distributions of final state particles produced in collisions at high energies , nucl .\n",
      "f. h. liu and j. s. li , isotopic production cross section of fragments in 56 @xmath34 and 136 124 pb reactions over an energy range from 300 to 1500 mev , phys .\n",
      "c. y. wong and g. wilk , tsallis fits to @xmath0 spectra and multiple hard scattering in @xmath35 collisions at the lhc , m. rybczyski , g. wilk and z. wlodarczyk , system size dependence of the log periodic oscillations of transverse momentum spectra , epj web conf .\n",
      "j. cleymans and d. worku , the tsallis distribution in proton proton collisions at @xmath36 0.9 tev at the lhc , j. phys .\n",
      "alice collaboration , production of pions , kaons and protons in @xmath35 collisions at @xmath37 gev with alice at the lhc , eur .\n",
      "li , comparing two boltzmann distribution and tsallis statistics of particle transverse mom tums in collisions at lhc energies , h. zheng , l. zhu and a. bonasera , systematic analysis of hadron spectra in collisions using tsallis distributions , phys .\n",
      "h. zheng and l. zhu , can tsallis distribution fit all the particle spectra produced at rhic and lhc , high energy phys .\n",
      "alice collaboration , centrality dependence of the nuclear modification factor of charged pions , kaons , and protons in pb pb collisions at @xmath38 tev , phys .\n",
      "b. svetitsky , diffusion of charmed quarks in the quark gluon plasma , phys .\n",
      "s. k. das , j. e. alam and p. mohanty , dragging heavy quarks in quark gluon plasma at the large hadron collider , phys .\n",
      "g. d. moore and d. teaney , how much do heavy quarks thermalize in a heavy ion collision 71 , 064904 2005 .\n",
      "s. k. das , j. e. alam and p. mohanty , probing quark gluon plasma properties by heavy flavours , phys .\n",
      "e. schnedermann , j. sollfrank and u. w. heinz , thermal phenomenology of hadrons from 200 gev collisions , phys .\n"
     ]
    }
   ],
   "source": [
    "# The article\n",
    "print('\\n'.join(example.article))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df.to_csv(\"data/arxiv_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2e25a6a6007380c2aaab295dbd93e746300d3d483f6c80ab0a58bac2da9fb50d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
